{"id": "131", "AI response": "```python\nimport string\n\ndef word_count(file_path):\n    \"\"\"\n    Counts the number of times each word appears in a file.\n\n    Args:\n        file_path (str): The path to the file.\n\n    Returns:\n        dict: A dictionary where the keys are the words and the values are the\n              number of times that word appears in the file. The dictionary is\n              sorted in descending order by the number of times the word\n              appears. If multiple words appear the same number of times, they\n              are sorted in ascending alphabetical order.\n    \"\"\"\n    word_counts = {}\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            line = line.strip().lower()\n            for char in string.punctuation:\n                line = line.replace(char, '')\n            words = line.split()\n            for word in words:\n                if word:\n                    word_counts[word] = word_counts.get(word, 0) + 1\n\n    sorted_word_counts = dict(sorted(word_counts.items(), key=lambda item: (-item[1], item[0])))\n\n    return sorted_word_counts\n```"}
{"id": "132", "AI response": "```python\nimport re\n\ndef remove_html_tags(s):\n    \"\"\"\n    Remove HTML tags, comments, and document type declarations from the input string.\n\n    Args:\n        s: The input string.\n\n    Returns:\n        The processed string, with HTML tags, comments, and document type declarations removed.\n    \"\"\"\n    if not isinstance(s, str):\n        raise ValueError(\"Input must be a string.\")\n\n    if not s:\n        return \"\"\n\n    tag_pattern = re.compile(r\"<[^>]+>\")\n    comment_pattern = re.compile(r\"<!--.*?-->\")\n    doctype_pattern = re.compile(r\"<!DOCTYPE.*?>\")\n\n    s = re.sub(comment_pattern, \"\", s)\n    s = re.sub(doctype_pattern, \"\", s)\n    s = re.sub(tag_pattern, \"\", s)\n\n    return s.strip()\n```"  }
{"id": "133", "AI response": "```python\nfrom datetime import datetime\n\nclass Schedule:\n    def __init__(self):\n        self.events = {}\n\n    def add_event(self, event_name, event_date):\n        if event_date in self.events:\n            self.events[event_date].append(event_name)\n        else:\n            self.events[event_date] = [event_name]\n\n    def view_events(self):\n        for event_date, event_list in self.events.items():\n            formatted_date = event_date.strftime(\"%Y-%m-%d %H:%M\")\n            print(f\"Date: {formatted_date}\")\n            for event in event_list:\n                print(f\"- {event}\")\n\n    def remove_event(self, event_name, event_date):\n        if event_date in self.events:\n            try:\n                self.events[event_date].remove(event_name)\n                if not self.events[event_date]:\n                    del self.events[event_date]\n            except ValueError:\n                print(f\"Event '{event_name}' not found\")\n        else:\n            print(f\"Event '{event_name}' not found\")\n```"}
{"id": "134", "AI response": "```python\nclass ListNode:\n    def __init__(self, value=0, next=None):\n        self.value = value\n        self.next = next\n\ndef add_two_numbers(l1, l2):\n    dummy_head = ListNode(0)\n    current = dummy_head\n    carry = 0\n\n    while l1 or l2 or carry:\n        x = l1.value if l1 else 0\n        y = l2.value if l2 else 0\n        sum_val = x + y + carry\n        carry = sum_val // 10\n        current.next = ListNode(sum_val % 10)\n        current = current.next\n\n        if l1:\n            l1 = l1.next\n        if l2:\n            l2 = l2.next\n\n    return dummy_head.next\n```"}
{"id": "135", "AI response": "```python\nimport xml.etree.ElementTree as ET\n\ndef parse_xml(file_path, tag_name):\n    \"\"\"\n    Parses an XML file and extracts data from specific tags.\n\n    Args:\n        file_path (str): The path to the XML file.\n        tag_name (str): The name of the tag to extract data from.\n\n    Returns:\n        list: A list containing the text content of all elements with the specified tag name.\n              Returns an empty list if parsing fails.\n    \"\"\"\n    try:\n        tree = ET.parse(file_path)\n        root = tree.getroot()\n    except ET.ParseError:\n        return []\n\n    data_list = []\n    for element in root.findall(tag_name):\n        data_list.append(element.text)\n\n    return data_list\n```"}
{"id": "136", "AI response": "```python\nimport re\n\ndef is_valid_variable_name(variable_name):\n    \"\"\"\n    Determines if a string is a valid JavaScript variable name.\n\n    Args:\n        variable_name: The string to check.\n\n    Returns:\n        True if the string is a valid variable name, False otherwise.\n    \"\"\"\n\n    if not isinstance(variable_name, str):\n        return False\n\n    if len(variable_name) > 255:\n        return False\n\n    if not re.match(r\"^[a-zA-Z_$][a-zA-Z0-9_$]*$\", variable_name):\n        return False\n\n    if variable_name and not (variable_name[0].isalpha() or variable_name[0] == '_' or variable_name[0] == '$'):\n        return False\n    \n    return True\n```"}
{"id": "137", "AI response": "```python\nclass Order:\n    def __init__(self, order_id):\n        self.order_id = order_id\n        self.items = []\n        self.total_price = 0.0\n        self.status = \"unpaid\"\n\n    def add_item(self, item_name, item_price):\n        self.items.append({\"name\": item_name, \"price\": item_price})\n\n    def remove_item(self, item_name):\n        for item in self.items:\n            if item[\"name\"] == item_name:\n                self.items.remove(item)\n                break\n\n    def calculate_total_price(self):\n        self.total_price = sum(item[\"price\"] for item in self.items)\n\n    def pay_order(self):\n        self.status = \"paid\"\n\n    def cancel_order(self):\n        self.status = \"cancelled\"\n\n    def view_order(self):\n        print(f\"Order number: {self.order_id}\")\n        print(\"Product list:\")\n        for item in self.items:\n            print(f\"- {item['name']}: ¥{item['price']}\")\n        print(f\"Total price: ¥{self.total_price}\")\n        print(f\"Status: {self.status}\")\n```"}
{"id": "138", "AI response": "```python\ndef is_valid_sequence(s):\n    \"\"\"\n    Determines whether the string meets the specified conditions.\n\n    Args:\n        s: The string to check.\n\n    Returns:\n        True if the string is valid, False otherwise.\n    \"\"\"\n\n    # Check the length of the string.\n    if not 2 <= len(s) <= 10:\n        return False\n\n    # Check if all characters are lowercase letters.\n    if not all('a' <= char <= 'z' for char in s):\n        return False\n\n    # Check for consecutive repeated characters.\n    for i in range(len(s) - 1):\n        if s[i] == s[i + 1]:\n            return False\n\n    # Check if the string can be divided into left and right parts that are either palindromes or increasing sequences.\n    for i in range(1, len(s)):\n        left = s[:i]\n        right = s[i:]\n\n        def is_palindrome(sub):\n            return sub == sub[::-1]\n\n        def is_increasing(sub):\n            for j in range(len(sub) - 1):\n                if ord(sub[j]) >= ord(sub[j + 1]):\n                    return False\n            return True\n\n        if (is_palindrome(left) and is_palindrome(right)) or \\\n           (is_increasing(left) and is_increasing(right)):\n            return True\n\n    return False\n```"}
{"id": "139", "AI response": "```python\nimport math\n\nclass RouteSimulator:\n    def __init__(self, start_x=0, start_y=0):\n        self.x = start_x\n        self.y = start_y\n        self.move_history = []\n\n    def move(self, distance, angle_degrees):\n        angle_radians = math.radians(angle_degrees)\n        new_x = self.x + distance * math.cos(angle_radians)\n        new_y = self.y + distance * math.sin(angle_radians)\n        self.x = new_x\n        self.y = new_y\n        self.move_history.append({\n            'distance': distance,\n            'angle_degrees': angle_degrees,\n            'position': (new_x, new_y)\n        })\n\n    def calculate_distance(self, x1, y1, x2, y2):\n        return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n\n    def print_current_position(self):\n        print(f\"当前位置：({self.x:.2f}, {self.y:.2f})\")\n\n    def print_move_history(self):\n        print(\"Move history:\")\n        for move in self.move_history:\n            distance = move['distance']\n            angle_degrees = move['angle_degrees']\n            position = move['position']\n            print(f\"Distance: {distance:.2f}, Angle: {angle_degrees:.2f}, Position: {position}\")\n```"}
{"id": "140", "AI response": "```python\ndef utf8_to_ascii(s):\n  \"\"\"\n  Converts a UTF-8 string to a list of ASCII codes.\n\n  Args:\n    s: The input string (UTF-8 encoded).\n\n  Returns:\n    A list of ASCII codes, where each element corresponds to the ASCII code\n    of a character in the input string.\n  \"\"\"\n  ascii_codes = []\n  for char in s:\n    ascii_codes.append(ord(char))\n  return ascii_codes\n```"}
{"id": "141", "AI response": "```python\nimport re\n\ndef find_dates(text):\n    \"\"\"\n    Finds all dates in the string in the format YYYY-MM-DD.\n\n    Args:\n        text: The string to search for dates in.\n\n    Returns:\n        A list of all dates found in the string.\n    \"\"\"\n    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n    dates = re.findall(date_pattern, text)\n    return dates\n```"}
{"id": "142", "AI response": "```python\nclass Process:\n    def __init__(self, pid, code):\n        self.pid = pid\n        self.code = code\n        self.memory = None\n\ndef allocate_memory(memory_map, process):\n    required_memory = len(process.code)\n    for i in range(len(memory_map) - required_memory + 1):\n        if all(memory_map[i+j] == 0 for j in range(required_memory)):\n            for j in range(required_memory):\n                memory_map[i+j] = process.pid\n            process.memory = (i, required_memory)\n            return True\n    return False\n\ndef deallocate_memory(memory_map, process):\n    if process.memory:\n        start, size = process.memory\n        for i in range(start, start + size):\n            memory_map[i] = 0\n        process.memory = None\n\ndef run_processes(memory_map, processes):\n    for process in processes:\n        if process.memory:\n            print(f\"Running process {process.pid}...\")\n            # Simulate running the process\n            deallocate_memory(memory_map, process)\n            print(f\"Process {process.pid} finished and memory deallocated.\")\n        else:\n            print(f\"Process {process.pid} was not allocated memory.\")\n\nif __name__ == '__main__':\n    # Example usage:\n    memory_map = [0] * 100  # Simulate 100 units of memory\n\n    process1 = Process(pid=1, code=[1, 2, 3, 4, 5])\n    process2 = Process(pid=2, code=[6, 7, 8])\n    process3 = Process(pid=3, code=[9, 10, 11, 12])\n\n    processes = [process1, process2, process3]\n\n    for process in processes:\n        if allocate_memory(memory_map, process):\n            print(f\"Allocated memory for process {process.pid} at {process.memory}\")\n        else:\n            print(f\"Failed to allocate memory for process {process.pid}\")\n\n    print(\"Memory Map:\", memory_map)\n    run_processes(memory_map, processes)\n    print(\"Memory Map after running processes:\", memory_map)\n```"}
{"id": "143", "AI response": "```python\nfrom bs4 import BeautifulSoup\nimport re\nfrom collections import Counter\n\ndef get_top_k_words_from_html(html, k):\n    \"\"\"\n    Returns the top k words with the highest frequency of occurrence in the given HTML text.\n\n    Args:\n        html (str): A string containing HTML tags.\n        k (int): A positive integer.\n\n    Returns:\n        list: A list containing the top k words with the highest frequency of occurrence and their frequencies, sorted in descending order of frequency.\n    \"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    text = soup.get_text()\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    word_counts = Counter(words)\n    top_k_words = word_counts.most_common(k)\n    return top_k_words\n```"}
{"id": "144", "AI response": "```python\nfrom typing import List\n\ndef extract_temperatures_from_text(text: str) -> List[float]:\n    \"\"\"\n    Extracts temperature data from the text.\n    Each line in the file contains a temperature value in Celsius.\n    \"\"\"\n    temperatures = []\n    for line in text.strip().split(\"\\n\"):\n        try:\n            temp_str = line.split(\",\")[1].replace(\"℃\", \"\").strip()\n            temperatures.append(float(temp_str))\n        except (IndexError, ValueError):\n            continue \n    return temperatures\n\ndef calculate_average_temperature(temperatures: List[float]) -> float:\n    \"\"\"\n    Calculates the average temperature.\n    \"\"\"\n    if not temperatures:\n        return 0.0\n    return sum(temperatures) / len(temperatures)\n\ndef find_highest_temperature(temperatures: List[float]) -> float:\n    \"\"\"\n    Finds the highest temperature.\n    \"\"\"\n    if not temperatures:\n        return float('-inf')\n    return max(temperatures)\n\ndef find_lowest_temperature(temperatures: List[float]) -> float:\n    \"\"\"\n    Finds the lowest temperature.\n    \"\"\"\n    if not temperatures:\n        return float('inf')\n    return min(temperatures)\n```"}
{"id": "145", "AI response": "```python\nimport re\n\ndef count_html_tags(html):\n    \"\"\"\n    Counts the number of HTML tags in a given HTML string.\n\n    Args:\n        html: The HTML string.\n\n    Returns:\n        The number of HTML tags in the string.\n    \"\"\"\n    tags = re.findall(r\"<[a-z]+>\", html)\n    return len(tags)\n```"}
{"id": "146", "AI response": "```python\nimport base64\nfrom Crypto.Cipher import AES\nfrom Crypto.Util.Padding import pad, unpad\n\ndef encrypt(plaintext, key):\n    \"\"\"\n    Encrypts plaintext using AES algorithm in ECB mode.\n\n    Args:\n        plaintext (str): The text to be encrypted.\n        key (bytes): The encryption key (must be 16, 24, or 32 bytes long).\n\n    Returns:\n        str: The Base64 encoded ciphertext.\n    \"\"\"\n    cipher = AES.new(key, AES.MODE_ECB)\n    padded_plaintext = pad(plaintext.encode('utf-8'), AES.block_size)\n    ciphertext = cipher.encrypt(padded_plaintext)\n    return base64.b64encode(ciphertext).decode('utf-8')\n\ndef decrypt(ciphertext, key):\n    \"\"\"\n    Decrypts ciphertext using AES algorithm in ECB mode.\n\n    Args:\n        ciphertext (str): The Base64 encoded ciphertext.\n        key (bytes): The decryption key (must be 16, 24, or 32 bytes long).\n\n    Returns:\n        str: The decrypted plaintext.\n    \"\"\"\n    cipher = AES.new(key, AES.MODE_ECB)\n    decoded_ciphertext = base64.b64decode(ciphertext)\n    decrypted_plaintext = unpad(cipher.decrypt(decoded_ciphertext), AES.block_size)\n    return decrypted_plaintext.decode('utf-8').rstrip()\n\ndef save_to_file(data, filename):\n    \"\"\"\n    Saves data to a file.\n\n    Args:\n        data (str): The data to be saved.\n        filename (str): The name of the file.\n    \"\"\"\n    with open(filename, 'w') as f:\n        f.write(data)\n\ndef read_from_file(filename):\n    \"\"\"\n    Reads data from a file.\n\n    Args:\n        filename (str): The name of the file.\n\n    Returns:\n        str: The data read from the file.\n    \"\"\"\n    with open(filename, 'r') as f:\n        return f.read()\n\nif __name__ == '__main__':\n    example_text = \"Hello, World!\"\n    encryption_key = b\"Sixteen byte key\"  # 16 bytes\n    encrypted_data = encrypt(example_text, encryption_key)\n    save_to_file(encrypted_data, \"encrypted_data.txt\")\n```"}
{"id": "147", "AI response": "```python\nimport os\nimport shutil\n\ndef backup_folder(source_folder, target_folder):\n    \"\"\"\n    Backs up all files in the source folder to the target folder, preserving the original file structure.\n\n    Args:\n        source_folder (str): The path to the source folder.\n        target_folder (str): The path to the target folder.\n    \"\"\"\n\n    # Source folder check\n    if not os.path.exists(source_folder):\n        raise FileNotFoundError(f\"Source folder '{source_folder}' does not exist.\")\n\n    # Target folder check\n    if not os.path.exists(target_folder):\n        os.makedirs(target_folder)\n\n    # Traverse all files and subfolders in the source folder\n    for root, _, files in os.walk(source_folder):\n        # Build target path\n        relative_path = os.path.relpath(root, source_folder)\n        target_path = os.path.join(target_folder, relative_path)\n\n        # Create the corresponding subfolders in the target folder\n        if not os.path.exists(target_path):\n            os.makedirs(target_path)\n\n        # File copy\n        for file in files:\n            source_file_path = os.path.join(root, file)\n            target_file_path = os.path.join(target_path, file)\n            shutil.copy2(source_file_path, target_file_path)\n\n    print(\"Backup completed successfully.\")\n```"}
{"id": "148", "AI response": "```python\nimport os\nimport shutil\n\ndef create_folder(folder_name):\n    \"\"\"Creates a folder if it doesn't exist.\"\"\"\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\ndef create_and_write_file(file_path, content):\n    \"\"\"Creates a file and writes the specified content to it.\"\"\"\n    try:\n        with open(file_path, 'w') as f:\n            f.write(content)\n    except Exception as e:\n        print(f\"Error creating and writing to file: {e}\")\n\ndef read_and_print_file(file_path):\n    \"\"\"Opens a file and reads and prints its content.\"\"\"\n    try:\n        with open(file_path, 'r') as f:\n            content = f.read()\n            print(content)\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n\ndef copy_file(source_path, destination_path):\n    \"\"\"Copies a file from the source path to the destination path.\"\"\"\n    try:\n        shutil.copyfile(source_path, destination_path)\n    except FileNotFoundError:\n        print(f\"Error: Source file not found at {source_path}\")\n    except Exception as e:\n        print(f\"Error copying file: {e}\")\n\ndef move_file(source_path, destination_path):\n    \"\"\"Moves a file from the source path to the destination path.\"\"\"\n    try:\n        shutil.move(source_path, destination_path)\n    except FileNotFoundError:\n        print(f\"Error: Source file not found at {source_path}\")\n    except Exception as e:\n        print(f\"Error moving file: {e}\")\n\ndef delete_folder(folder_name):\n    \"\"\"Deletes a folder and all its contents.\"\"\"\n    try:\n        if os.path.exists(folder_name):\n            shutil.rmtree(folder_name)\n    except Exception as e:\n        print(f\"Error deleting folder: {e}\")\n```"}
{"id": "149", "AI response": "```python\nimport re\n\ndef extract_phone_numbers(text):\n    \"\"\"\n    Extracts phone numbers from a given text string and formats them.\n\n    Args:\n        text (str): The input text string.\n\n    Returns:\n        list: A list of formatted phone numbers found in the text.\n    \"\"\"\n    pattern = re.compile(r'\\b\\(?(\\d{3})\\)?[-.\\s]?(\\d{3})[-.\\s]?(\\d{4})\\b')\n    matches = pattern.finditer(text)\n    phone_numbers = []\n    for match in matches:\n        area_code, middle, end = match.groups()\n        formatted_number = f\"({area_code}) {middle}-{end}\"\n        phone_numbers.append(formatted_number)\n    return phone_numbers\n```"}
{"id": "150", "AI response": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef find_max_eigenvalues(matrix, num_largest=5):\n    \"\"\"\n    Calculates the maximum eigenvalues and corresponding eigenvectors of a two-dimensional matrix.\n\n    Args:\n        matrix (numpy.ndarray): A two-dimensional array representing the matrix.\n        num_largest (int, optional): The number of maximum eigenvalues to be extracted. Defaults to 5.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - A list of the maximum eigenvalues.\n            - A list of the corresponding eigenvectors.\n\n    Raises:\n        ValueError: If the input matrix is empty or incorrectly formatted.\n    \"\"\"\n    if not isinstance(matrix, np.ndarray):\n        try:\n            matrix = np.array(matrix)\n        except:\n            raise ValueError(\"Input matrix could not be converted to a numpy array.\")\n\n    if matrix.size == 0:\n        raise ValueError(\"Input matrix cannot be empty.\")\n\n    if matrix.ndim != 2:\n        raise ValueError(\"Input matrix must be a two-dimensional array.\")\n\n    try:\n        eigenvalues, eigenvectors = eigh(matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"Failed to calculate eigenvalues. Ensure the matrix is square and contains numerical values.\")\n\n    eigenvalues = eigenvalues[-num_largest:]\n    eigenvectors = eigenvectors[:, -num_largest:]\n\n    return list(eigenvalues), eigenvectors.T.tolist()\n\ndef calculate_eigen(matrix, num_largest=5):\n    \"\"\"\n    Calculates the maximum eigenvalues and corresponding eigenvectors of a two-dimensional matrix.\n\n    Args:\n        matrix (numpy.ndarray): A two-dimensional array representing the matrix.\n        num_largest (int, optional): The number of maximum eigenvalues to be extracted. Defaults to 5.\n\n    Returns:\n        tuple: A tuple containing two lists:\n            - A list of the maximum eigenvalues.\n            - A list of the corresponding eigenvectors.\n\n    Raises:\n        ValueError: If the input matrix is empty or incorrectly formatted.\n    \"\"\"\n    if not isinstance(matrix, np.ndarray):\n        try:\n            matrix = np.array(matrix)\n        except:\n            raise ValueError(\"Input matrix could not be converted to a numpy array.\")\n\n    if matrix.size == 0:\n        raise ValueError(\"Input matrix cannot be empty.\")\n\n    if matrix.ndim != 2:\n        raise ValueError(\"Input matrix must be a two-dimensional array.\")\n\n    try:\n        eigenvalues, eigenvectors = eigh(matrix)\n    except np.linalg.LinAlgError:\n        raise ValueError(\"Failed to calculate eigenvalues. Ensure the matrix is square and contains numerical values.\")\n\n    eigenvalues = eigenvalues[-num_largest:]\n    eigenvectors = eigenvectors[:, -num_largest:]\n\n    return list(eigenvalues), eigenvectors.T.tolist()\n```"}
{"id": "151", "AI response": "```python\ndef flatten_list(nested_list):\n    flattened_list = []\n    stack = [nested_list]\n    while stack:\n        current = stack.pop()\n        for element in reversed(current):\n            if isinstance(element, list):\n                stack.append(element)\n            else:\n                flattened_list.append(element)\n    return flattened_list[::-1]\n```"}
{"id": "152", "AI response": "```python\nimport openpyxl\n\ndef txt2excle(file_path):\n    \"\"\"\n    Converts a tab-separated text file to an Excel spreadsheet.\n\n    Args:\n        file_path (str): The path to the text file.\n    \"\"\"\n    workbook = openpyxl.Workbook()\n    sheet = workbook.active\n\n    with open(file_path, 'r') as f:\n        for row_index, line in enumerate(f, start=1):\n            values = line.strip().split('\\t')\n            for col_index, value in enumerate(values, start=1):\n                sheet.cell(row=row_index, column=col_index, value=value)\n\n    workbook.save('output.xlsx')\n```"}
{"id": "153", "AI response": "```python\nimport xml.etree.ElementTree as ET\n\ndef calculate_iou(box1, box2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) of two bounding boxes.\n\n    Args:\n        box1 (tuple): (xmin, ymin, xmax, ymax) of the first bounding box.\n        box2 (tuple): (xmin, ymin, xmax, ymax) of the second bounding box.\n\n    Returns:\n        float: IoU value.\n    \"\"\"\n    xmin1, ymin1, xmax1, ymax1 = box1\n    xmin2, ymin2, xmax2, ymax2 = box2\n\n    # Calculate the coordinates of the intersection rectangle\n    x_left = max(xmin1, xmin2)\n    y_top = max(ymin1, ymin2)\n    x_right = min(xmax1, xmax2)\n    y_bottom = min(ymax1, ymax2)\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # Calculate the area of intersection rectangle\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # Calculate the area of both bounding boxes\n    box1_area = (xmax1 - xmin1) * (ymax1 - ymin1)\n    box2_area = (xmax2 - xmin2) * (ymax2 - ymin2)\n\n    # Calculate the IoU\n    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n    return iou\n\n\ndef convert_labels(xml_file, target_box):\n    \"\"\"\n    Convert object names to '0' if their IoU with the target box is greater than 0.2.\n\n    Args:\n        xml_file (str): Path to the XML file in VOC format.\n        target_box (tuple): (xmin, ymin, xmax, ymax) of the target bounding box.\n    \"\"\"\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n\n    for obj in root.findall('object'):\n        bbox = obj.find('bndbox')\n        xmin = int(bbox.find('xmin').text)\n        ymin = int(bbox.find('ymin').text)\n        xmax = int(bbox.find('xmax').text)\n        ymax = int(bbox.find('ymax').text)\n\n        object_box = (xmin, ymin, xmax, ymax)\n        iou = calculate_iou(object_box, target_box)\n\n        if iou > 0.2:\n            obj.find('name').text = '0'\n\n    tree.write(xml_file)\n```"}
{"id": "154", "AI response": "```python\nimport os\nimport random\nimport shutil\n\ndef split_data(dataset_path, train_ratio, test_ratio, val_ratio, random_seed=42):\n    \"\"\"\n    Divides an image classification dataset into training, test, and validation sets.\n\n    Args:\n        dataset_path (str): Path to the image folder.\n        train_ratio (float): Ratio of images for the training set.\n        test_ratio (float): Ratio of images for the test set.\n        val_ratio (float): Ratio of images for the validation set.\n        random_seed (int): Random seed for shuffling and splitting.\n    \"\"\"\n\n    if not os.path.exists(dataset_path):\n        raise ValueError(f\"Dataset path '{dataset_path}' does not exist.\")\n\n    if not (train_ratio + test_ratio + val_ratio == 1.0):\n        raise ValueError(\"Train, test, and validation ratios must sum to 1.0.\")\n\n    random.seed(random_seed)\n\n    # Create output directories\n    train_dir = os.path.join(dataset_path, 'train')\n    test_dir = os.path.join(dataset_path, 'test')\n    val_dir = os.path.join(dataset_path, 'val')\n\n    os.makedirs(train_dir, exist_ok=True)\n    os.makedirs(test_dir, exist_ok=True)\n    os.makedirs(val_dir, exist_ok=True)\n\n    # Get list of all files in the dataset directory\n    all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n\n    # Shuffle the files randomly\n    random.shuffle(all_files)\n\n    # Calculate the number of files for each set\n    num_files = len(all_files)\n    num_train = int(num_files * train_ratio)\n    num_test = int(num_files * test_ratio)\n    num_val = num_files - num_train - num_test\n\n    # Split the files into training, test, and validation sets\n    train_files = all_files[:num_train]\n    test_files = all_files[num_train:num_train + num_test]\n    val_files = all_files[num_train + num_test:]\n\n    # Move the files to their respective directories\n    for file in train_files:\n        src_path = os.path.join(dataset_path, file)\n        dst_path = os.path.join(train_dir, file)\n        shutil.move(src_path, dst_path)\n\n    for file in test_files:\n        src_path = os.path.join(dataset_path, file)\n        dst_path = os.path.join(test_dir, file)\n        shutil.move(src_path, dst_path)\n\n    for file in val_files:\n        src_path = os.path.join(dataset_path, file)\n        dst_path = os.path.join(val_dir, file)\n        shutil.move(src_path, dst_path)\n```"}
{"id": "155", "AI response": "```python\nimport numpy as np\n\ndef hinge_loss(y_true, y_pred):\n    \"\"\"\n    Calculates the average hinge loss.\n\n    Args:\n        y_true (np.ndarray): Ground truth labels (1 or -1).\n        y_pred (np.ndarray): Predicted labels.\n\n    Returns:\n        float: The average hinge loss.\n    \"\"\"\n    loss = 1 - y_true * y_pred\n    loss[loss < 0] = 0\n    return np.mean(loss)\n```"}
{"id": "156", "AI response": "```python\nimport os\n\ndef remove_empty_lines_folder(folder_path):\n    \"\"\"\n    Removes empty lines from all text files in a folder.\n\n    Args:\n        folder_path (str): The path to the folder containing the text files.\n    \"\"\"\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".txt\"):\n            file_path = os.path.join(folder_path, filename)\n            with open(file_path, 'r') as file:\n                lines = file.readlines()\n\n            non_empty_lines = [line for line in lines if line.strip()]\n\n            with open(file_path, 'w') as file:\n                file.writelines(non_empty_lines)\n```"}
{"id": "157", "AI response": "```python\nimport os\n\ndef label_filter(folder_path, Label_category):\n    \"\"\"\n    Filter the category in the annotation files of the yolo format dataset,\n    only retain the category labels in Label_category, and delete other category labels,\n    operate on the original file.\n    Args:\n        folder_path: the path of the annotation file folder of the yolo format dataset\n        Label_category: the Label_category list\n    \"\"\"\n    for filename in os.listdir(folder_path):\n        if filename.endswith(\".txt\"):\n            file_path = os.path.join(folder_path, filename)\n            with open(file_path, \"r\") as f:\n                lines = f.readlines()\n\n            filtered_lines = []\n            deleted_lines = []\n            for line in lines:\n                category = int(line.split()[0])\n                if category in Label_category:\n                    deleted_lines.append(line)\n                else:\n                    filtered_lines.append(line)\n            \n            with open(file_path, \"w\") as f:\n                for line in filtered_lines:\n                    f.write(line)\n```"}
{"id": "158","AI response": "```python\nimport os\n\ndef w_h_filter(annotation_folder, limit_wideth, limit_height):\n    \"\"\"\n    Filters YOLO format annotation files, removing lines where width and height are below the specified limits.\n    Operates directly on the original files.\n\n    Args:\n        annotation_folder (str): Path to the folder containing YOLO annotation files.\n        limit_wideth (float): Minimum allowed width.\n        limit_height (float): Minimum allowed height.\n    \"\"\"\n    for filename in os.listdir(annotation_folder):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(annotation_folder, filename)\n            with open(filepath, \"r\") as f:\n                lines = f.readlines()\n\n            filtered_lines = []\n            for line in lines:\n                parts = line.strip().split()\n                if len(parts) == 5:\n                    try:\n                        width = float(parts[3])\n                        height = float(parts[4])\n                        if width > limit_wideth and height > limit_height:\n                            filtered_lines.append(line)\n                    except ValueError:\n                        # Handle cases where conversion to float fails (e.g., corrupted data)\n                        print(f\"Skipping line due to ValueError: {line.strip()} in file: {filename}\")\n                        continue  # Skip the problematic line\n\n                else:\n                    print(f\"Skipping line due to incorrect number of fields: {line.strip()} in file: {filename}\")\n                    continue # skip the problematic line\n\n            with open(filepath, \"w\") as f:\n                f.writelines(filtered_lines)\n```"}
{"id": "159", "AI response": "```python\nimport numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    \"\"\"\n    Calculates the mean squared error between two arrays.\n\n    Args:\n        y_true (np.ndarray): The ground truth values.\n        y_pred (np.ndarray): The predicted values.\n\n    Returns:\n        float: The mean squared error.\n    \"\"\"\n    return np.mean((y_true - y_pred)**2)\n```"}
{"id": "160", "AI response": "```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\ndef process_sentence(sentence):\n    \"\"\"\n    Performs tokenization, part-of-speech tagging, stop word removal,\n    and stem extraction on a sentence.\n\n    Args:\n        sentence (str): The sentence to process.\n\n    Returns:\n        tuple: A tuple containing the results of each operation:\n            - tokenized_words (list): List of tokens.\n            - pos_tagged_words (list): List of (token, POS tag) tuples.\n            - stop_words_removed (list): List of tokens after stop word removal.\n            - stemmed_words (list): List of stemmed tokens.\n    \"\"\"\n\n    # Tokenization\n    tokenized_words = word_tokenize(sentence)\n\n    # Part-of-speech tagging\n    pos_tagged_words = nltk.pos_tag(tokenized_words)\n\n    # Stop word removal\n    stop_words = set(stopwords.words('english'))\n    stop_words_removed = [w.lower() for w in tokenized_words if w.lower() not in stop_words]\n\n    # Stem extraction\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(w.lower()) for w in tokenized_words]\n\n    return tokenized_words, pos_tagged_words, [w.lower() for w in tokenized_words], stemmed_words\n```"}
{"id": "161", "AI response": "```python\nimport pandas as pd\n\ndef calculate_and_save_probabilities(input_csv, output_csv):\n    \"\"\"\n    Calculates the probability of survival given sex and saves the one-hot encoded DataFrame.\n\n    Args:\n        input_csv (str): Path to the input CSV file.\n        output_csv (str): Path to the output CSV file.\n\n    Returns:\n        tuple: (probability of female survival, probability of male survival)\n    \"\"\"\n    df = pd.read_csv(input_csv)\n\n    # Calculate probabilities\n    total_female = df['Sex'][df['Sex'] == 'female'].count()\n    female_survived = df['Survived'][(df['Sex'] == 'female') & (df['Survived'] == 1)].count()\n    prob_female_survival = female_survived / total_female if total_female > 0 else 0.0\n\n    total_male = df['Sex'][df['Sex'] == 'male'].count()\n    male_survived = df['Survived'][(df['Sex'] == 'male') & (df['Survived'] == 1)].count()\n    prob_male_survival = male_survived / total_male if total_male > 0 else 0.0\n\n    # Convert non-numeric columns to one-hot encoding\n    df = pd.get_dummies(df, columns=['Sex'])\n\n    # Save the DataFrame to a new CSV file\n    df.to_csv(output_csv, index=False)\n\n    return (prob_female_survival, prob_male_survival)\n```"}
{"id": "162", "AI response": "```python\nimport numpy as np\n\ndef linear_fit(x, y):\n    \"\"\"\n    Given two vectors of equal length x, y, use the numpy toolkit to solve for a and b using the least squares method for linear fitting, and find the slope a, intercept b, and correlation coefficient r.\n    Function usage: a, b, r = linear_fit(x, y)\n    \"\"\"\n    x = np.array(x)\n    y = np.array(y)\n    n = len(x)\n    \n    # Calculate the slope (a) and intercept (b)\n    a = (n * np.sum(x * y) - np.sum(x) * np.sum(y)) / (n * np.sum(x**2) - np.sum(x)**2)\n    b = (np.sum(y) - a * np.sum(x)) / n\n    \n    # Calculate the correlation coefficient (r)\n    y_predicted = a * x + b\n    \n    numerator = np.sum((y - np.mean(y)) * (y_predicted - np.mean(y_predicted)))\n    denominator_x = np.sqrt(np.sum((y - np.mean(y))**2))\n    denominator_y = np.sqrt(np.sum((y_predicted - np.mean(y_predicted))**2))\n    \n    r = numerator / (denominator_x * denominator_y)\n    \n    return a, b, r\n```"}
{"id": "163", "AI response": "```python\nimport os\n\nclass list_files:\n    def __init__(self, path):\n        self.path = path\n        self.list_files()\n\n    def list_files(self):\n        if not os.path.exists(self.path):\n            print(\"Invalid directory path.\")\n            return\n\n        for root, _, files in os.walk(self.path):\n            for file in files:\n                print(file)\n```"}
{"id": "164", "AI response": "```python\nimport os\n\ndef find_file(filename, path):\n    \"\"\"\n    Checks if a file exists in the specified directory.\n\n    Args:\n        filename (str): The name of the file to search for.\n        path (str): The path to the directory to search in.\n    \"\"\"\n    found_files = []\n    for root, _, files in os.walk(path):\n        if filename in files:\n            found_files.append(os.path.abspath(os.path.join(root, filename)))\n\n    if not found_files:\n        print(\"The file is not on disk\")\n    else:\n        for file_path in found_files:\n            print(file_path)\n```"}
{"id": "165", "AI response": "```python\nimport csv\n\ndef calculate_average(file_path, column_index):\n    \"\"\"\n    Calculates the average of a specified column in a CSV file.\n\n    Args:\n        file_path (str): The path to the CSV file.\n        column_index (int): The index of the column to average (0-based).\n\n    Returns:\n        float: The average of the specified column, or None if the file is empty or an error occurs.\n    \"\"\"\n    try:\n        with open(file_path, 'r') as csvfile:\n            reader = csv.reader(csvfile)\n            next(reader, None)  # Skip the header row.\n\n            column_values = []\n            for row in reader:\n                try:\n                    value = float(row[column_index])\n                    column_values.append(value)\n                except (ValueError, IndexError):\n                    # Handle cases where the value is not a number or the column doesn't exist in the row.\n                    pass  # or log the error\n\n            if column_values:\n                return sum(column_values) / len(column_values)\n            else:\n                return None  # Return None if the column is empty\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n```"}
{"id": "166", "AI response": "```python\nimport pandas as pd\nimport os\n\ndef find_max_temperature(data_folder, output_file):\n    \"\"\"\n    Finds the highest temperature and corresponding date for each city from CSV files\n    in the data folder and saves the results to a new CSV file.\n\n    Args:\n        data_folder (str): Path to the folder containing the CSV files.\n        output_file (str): Path to the output CSV file.\n    \"\"\"\n\n    results = []\n    for filename in os.listdir(data_folder):\n        if filename.endswith(\".csv\"):\n            city = filename[:-4]  # Extract city name from filename\n            file_path = os.path.join(data_folder, filename)\n            df = pd.read_csv(file_path)\n            max_temp = df['Temperature'].max()\n            max_temp_dates = df[df['Temperature'] == max_temp]['Date'].tolist()\n            \n            for date in max_temp_dates:\n                results.append({'City': city, 'Max_Temperature': max_temp, 'Date': date})\n\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(output_file, index=False, header=['City', 'Max_Temperature', 'Date'])\n```"}
{"id": "167", "AI response": "```python\nimport pandas as pd\n\ndef merge_and_smooth_data(file_list, window_size):\n    \"\"\"\n    Reads data from multiple CSV files, merges them based on timestamp,\n    and calculates the moving average of the merged data.\n\n    Args:\n        file_list (list): A list of CSV file paths.\n        window_size (int): The size of the moving average window.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing the moving average of the merged data.\n    \"\"\"\n    dfs = []\n    for file in file_list:\n        df = pd.read_csv(file)\n        dfs.append(df)\n\n    merged_df = pd.concat(dfs, ignore_index=True)\n    merged_df['timestamp'] = pd.to_datetime(merged_df['timestamp'])\n    merged_df = merged_df.sort_values(by='timestamp')\n    merged_df = merged_df.set_index('timestamp')\n\n    numeric_columns = merged_df.select_dtypes(include=['number']).columns\n    smoothed_data = merged_df[numeric_columns].rolling(window=window_size).mean().dropna()\n\n    return smoothed_data\n```"}
{"id": "168", "AI response": "```python\nimport os\nfrom PIL import Image\n\ndef resize_and_rename_images(input_folder, output_folder, resolution):\n    \"\"\"\n    Converts images in a folder to RGB, resizes them, and renames them sequentially.\n\n    Args:\n        input_folder: Path to the folder containing the original images.\n        output_folder: Path to the folder where the modified images will be saved.\n        resolution: A tuple (width, height) specifying the desired resolution.\n    \"\"\"\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    image_paths = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))]\n    image_paths.sort()  # Sort images alphabetically\n\n    modified_images_resolutions = {}\n    \n    for i, image_path in enumerate(image_paths):\n        try:\n            img = Image.open(image_path).convert(\"RGB\")\n            img = img.resize(resolution)\n            new_filename = f\"{i+1:03d}.jpg\"  # Format as 001.jpg, 002.jpg, etc.\n            new_filepath = os.path.join(output_folder, new_filename)\n            img.save(new_filepath)\n            modified_images_resolutions[new_filename] = resolution\n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n\n    print(\"Modified images resolutions:\")\n    for filename, resolution in modified_images_resolutions.items():\n        print(f\"{filename}: {resolution[0]} x {resolution[1]}\")\n```"}
{"id": "169", "AI response": "```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Compute the focal loss between `inputs` and `targets`.\n\n        Args:\n            inputs: A float tensor of arbitrary shape.\n                    The predictions for each example.\n            targets: A float tensor with the same shape as inputs. Stores the binary\n                     classification label for each element in `inputs`\n                    (0 for the negative class and 1 for the positive class).\n\n        Returns:\n            Loss tensor\n        \"\"\"\n        logpt = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n        pt = torch.exp(-logpt)\n        loss = -self.alpha * (1 - pt) ** self.gamma * targets * logpt - (1 - self.alpha) * (1 - pt) ** self.gamma * (1 - targets) * logpt\n        return torch.mean(loss)\n```"}
{"id": "170", "AI response": "```python\nimport os\nimport shutil\n\ndef copy_and_move_file(source_path, destination_folder):\n    \"\"\"\n    Copies a file from the source path to the destination folder.\n\n    Args:\n        source_path: The path to the file to copy.\n        destination_folder: The path to the folder where the file should be copied.\n    \"\"\"\n    try:\n        shutil.copy(source_path, destination_folder)\n        # Construct the destination path\n        destination_path = os.path.join(destination_folder, os.path.basename(source_path))\n        # If file exist, do nothing\n        # shutil.move(source_path, destination_path)\n    except FileNotFoundError:\n        print(\"File not exist.\")\n    except OSError as e:\n        print(f\"Error moving/copying file: {e}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n```"}
{"id": "171", "AI response": "```python\nfrom docx import Document\n\ndef count_keywords_in_docx(docx_path, keywords):\n    \"\"\"\n    Reads a Word document, merges all paragraph texts into a single string,\n    and counts the occurrences of each keyword in the string.\n\n    Args:\n        docx_path (str): The path to the Word document.\n        keywords (list): A list of keywords to count.\n\n    Returns:\n        dict: A dictionary where keys are keywords and values are their counts.\n    \"\"\"\n    doc = Document(docx_path)\n    full_text = []\n    for paragraph in doc.paragraphs:\n        full_text.append(paragraph.text)\n    text = ' '.join(full_text)\n    keyword_counts = {}\n    for keyword in keywords:\n        keyword_counts[keyword] = text.count(keyword)\n    return keyword_counts\n```"}
{"id": "172", "AI response": "```python\nimport os\nimport shutil\n\ndef delete_folder(path):\n    \"\"\"Deletes all files in the specified path.\n\n    Args:\n        path: The path to the folder.\n\n    Returns:\n        \"Successfully delete the folder.\" if successfully deleted,\n        \"Can't delete the folder.\" otherwise.\n    \"\"\"\n    try:\n        shutil.rmtree(path)\n        return \"Successfully delete the folder.\"\n    except OSError:\n        return \"Can't delete the folder.\"\n```"}
{"id": "173", "AI response": "```python\nimport scipy.sparse\nimport numpy as np\n\ndef csr_to_numpy(npz_file, npy_file):\n    \"\"\"\n    Convert a scipy.sparse.csr_matrix to ndarray and save it in a npy file.\n\n    Args:\n        npz_file (str): Path to the npz file containing the csr_matrix.\n        npy_file (str): Path to save the ndarray in npy format.\n    \"\"\"\n    csr_matrix = scipy.sparse.load_npz(npz_file)\n    ndarray = csr_matrix.toarray()\n    np.save(npy_file, ndarray)\n```"}
{"id": "174", "AI response": "```python\nimport os\nimport shutil\nimport threading\nimport time\n\ndef copy_folder(source, destination):\n    \"\"\"\n    Copies a folder from source to destination.\n    \"\"\"\n    try:\n        shutil.copytree(source, destination)\n    except FileExistsError:\n        # If the destination folder already exists, remove it and try again.\n        shutil.rmtree(destination)\n        shutil.copytree(source, destination)\n    except Exception as e:\n        print(f\"Error copying {source} to {destination}: {e}\")\n\ndef multithread_copy(source_folder, dest_folders):\n    \"\"\"\n    Copies a source folder to multiple destination folders using multithreading.\n\n    Args:\n        source_folder (str): The path to the source folder.\n        dest_folders (list): A list of paths to the destination folders.\n    \"\"\"\n\n    start_time = time.time()\n    threads = []\n\n    for dest_folder in dest_folders:\n        thread = threading.Thread(target=copy_folder, args=(source_folder, dest_folder))\n        threads.append(thread)\n        thread.start()\n\n    for thread in threads:\n        thread.join()\n\n    end_time = time.time()\n    duration = end_time - start_time\n    print(f\"Copying completed in {duration:.2f} seconds.\")\n```"}
{"id": "175", "AI response": "```python\nimport numpy as np\nimport scipy.sparse as sp\n\ndef numpy_to_csr(npy_path, npz_path):\n    \"\"\"\n    Converts a sparse ndarray from an npy file to a csr_matrix and saves it as an npz file.\n\n    Args:\n        npy_path (str): Path to the input npy file.\n        npz_path (str): Path to the output npz file.\n    \"\"\"\n    arr = np.load(npy_path)\n    csr_matrix = sp.csr_matrix(arr)\n    sp.save_npz(npz_path, csr_matrix)\n```"}
{"id": "176", "AI response": "```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef calculate_similarity(matrix_A_path, matrix_B_path, output_path):\n    \"\"\"\n    Calculate the cosine similarity of two matrices using sklearn.\n\n    Args:\n        matrix_A_path (str): Path to the numpy file containing matrix A.\n        matrix_B_path (str): Path to the numpy file containing matrix B.\n        output_path (str): Path to save the cosine similarity matrix.\n    \"\"\"\n    A = np.load(matrix_A_path)\n    B = np.load(matrix_B_path)\n    similarity_matrix = cosine_similarity(A, B)\n    np.save(output_path, similarity_matrix)\n```"}
{"id": "177", "AI response": "```python\nimport numpy as np\nimport torch\n\ndef process_tensor(array1, array2):\n    \"\"\"\n    Converts two numpy arrays of shape (X, Y, Z) into tensor type and single-precision floating point type.\n    Changes the shape of one of them to (X, Z, Y).\n    Calculates the attention scores of these two matrices and outputs the results.\n    Then writes the results into a file named result.txt.\n    \"\"\"\n    tensor1 = torch.tensor(array1, dtype=torch.float32)\n    tensor2 = torch.tensor(array2, dtype=torch.float32)\n    tensor2 = tensor2.permute(0, 2, 1)\n    \n    attention_scores = torch.matmul(tensor1, tensor2)\n    \n    with open(\"result.txt\", \"w\") as f:\n        for i in range(attention_scores.shape[0]):\n            for j in range(attention_scores.shape[1]):\n                for k in range(attention_scores.shape[2]):\n                    f.write(str(attention_scores[i, j, k].item()) + \"\\n\")\n                    \n    return attention_scores\n```"}
{"id": "178", "AI response": "```python\nimport math\n\ndef euclidean_distance(point1, point2):\n    \"\"\"Calculates the Euclidean distance between two points.\"\"\"\n    distance = 0\n    for i in range(len(point1)):\n        distance += (point1[i] - point2[i]) ** 2\n    return math.sqrt(distance)\n\ndef manhattan_distance(point1, point2):\n    \"\"\"Calculates the Manhattan distance between two points.\"\"\"\n    distance = 0\n    for i in range(len(point1)):\n        distance += abs(point1[i] - point2[i])\n    return distance\n\ndef chebyshev_distance(point1, point2):\n    \"\"\"Calculates the Chebyshev distance between two points.\"\"\"\n    distance = 0\n    for i in range(len(point1)):\n        distance = max(distance, abs(point1[i] - point2[i]))\n    return distance\n\ndef minkowski_distance(point1, point2, p):\n    \"\"\"Calculates the Minkowski distance between two points.\"\"\"\n    distance = 0\n    for i in range(len(point1)):\n        distance += abs(point1[i] - point2[i]) ** p\n    return distance ** (1 / p)\n\ndef cosine_similarity(vector1, vector2):\n    \"\"\"Calculates the cosine similarity between two vectors.\"\"\"\n    dot_product = 0\n    magnitude1 = 0\n    magnitude2 = 0\n    for i in range(len(vector1)):\n        dot_product += vector1[i] * vector2[i]\n        magnitude1 += vector1[i] ** 2\n        magnitude2 += vector2[i] ** 2\n    return dot_product / (math.sqrt(magnitude1) * math.sqrt(magnitude2))\n\ndef hamming_distance(string1, string2):\n    \"\"\"Calculates the Hamming distance between two strings.\"\"\"\n    distance = 0\n    for i in range(len(string1)):\n        if string1[i] != string2[i]:\n            distance += 1\n    return distance\n\ndef jaccard_similarity(set1, set2):\n    \"\"\"Calculates the Jaccard similarity between two sets.\"\"\"\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union\n```"}
{"id": "179", "AI response": "```python\nimport numpy as np\n\ndef diag_zero(matrix):\n    \"\"\"\n    Sets the values above the diagonal of a square matrix to zero,\n    and returns the sum of the remaining values.\n\n    Args:\n        matrix (numpy.ndarray): A square matrix.\n\n    Returns:\n        int: The sum of the values on and below the diagonal.\n    \"\"\"\n    n = matrix.shape[0]\n    for i in range(n):\n        for j in range(i + 1, n):\n            matrix[i, j] = 0\n    return np.sum(matrix)\n```"}
{"id": "180", "AI response": "```python\nimport numpy as np\n\ndef array_avg(matrix):\n    mean_value = np.mean(matrix)\n    new_matrix = np.where(matrix > mean_value, matrix, 1)\n    flattened_array = new_matrix.flatten()\n    return mean_value, flattened_array\n```"} 
{"id": "181", "AI response": "```python\nimport os\n\ndef count_files(dir_path):\n    \"\"\"\n    Calculates the total number of files in a directory and its subdirectories.\n\n    Args:\n        dir_path (str): The path to the directory.\n\n    Returns:\n        int: The total number of files.\n    \"\"\"\n    total_files = 0\n    for root, _, files in os.walk(dir_path):\n        total_files += len(files)\n    return total_files\n```"}
{"id": "182", "AI response": "```python\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\ndef pac_fit(file_path):\n    \"\"\"\n    Applies PCA to reduce the dimensions of the input DataFrame and returns the explained variance ratio.\n\n    Args:\n        file_path (str): The path to the CSV file containing the character data.\n\n    Returns:\n        numpy.ndarray: The explained variance ratio for each principal component.\n    \"\"\"\n    characters = pd.read_csv(file_path)\n    pca = PCA()\n    pca.fit(characters)\n    return pca.explained_variance_ratio_\n```"}
{"id": "183", "AI response": "```python\nimport os\nimport shutil\nimport zipfile\nimport time\nimport datetime\n\ndef get_dir_size(path):\n    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size\n\ndef delete_oldest_backup(backup_folder, threshold_mb):\n    threshold_bytes = threshold_mb * 1024 * 1024\n    total_size = get_dir_size(backup_folder)\n    if total_size > threshold_bytes:\n        files = sorted([os.path.join(backup_folder, f) for f in os.listdir(backup_folder) if f.endswith(\".zip\")], key=os.path.getmtime)\n        while total_size > threshold_bytes and files:\n            oldest_file = files.pop(0)\n            file_size = os.path.getsize(oldest_file)\n            os.remove(oldest_file)\n            total_size -= file_size\n            print(f\"Deleted oldest backup: {oldest_file}\")\n\ndef backup(source_path, backup_folder, threshold_mb=100):  # threshold_mb = 100MB by default\n    \"\"\"\n    Backs up the contents of the source path to the backup folder,\n    including a timestamp in the backup filename, compressing the backup,\n    and deleting the oldest backups if the backup folder exceeds the threshold.\n    \"\"\"\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    backup_filename = f\"backup_{timestamp}.zip\"\n    backup_filepath = os.path.join(backup_folder, backup_filename)\n\n    with zipfile.ZipFile(backup_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                filepath = os.path.join(root, file)\n                zipf.write(filepath, os.path.relpath(filepath, source_path))\n\n    delete_oldest_backup(backup_folder, threshold_mb)\n```"}
{"id": "184", "AI response": "```python\nimport torch\n\ndef bbox_iou(boxes1, boxes2):\n    \"\"\"\n    Calculates the Intersection over Union (IoU) of two sets of bounding boxes.\n\n    Args:\n        boxes1 (torch.Tensor): Tensor of shape [N, 4] representing N bounding boxes in the format [x, y, w, h].\n        boxes2 (torch.Tensor): Tensor of shape [N, 4] representing N bounding boxes in the format [x, y, w, h].\n\n    Returns:\n        torch.Tensor: Tensor of shape [N] containing the IoU values for each pair of bounding boxes.\n    \"\"\"\n\n    x1 = boxes1[:, 0]\n    y1 = boxes1[:, 1]\n    w1 = boxes1[:, 2]\n    h1 = boxes1[:, 3]\n\n    x2 = boxes2[:, 0]\n    y2 = boxes2[:, 1]\n    w2 = boxes2[:, 2]\n    h2 = boxes2[:, 3]\n\n    # Calculate coordinates of top-left and bottom-right corners for each box\n    x1_min = x1\n    y1_min = y1\n    x1_max = x1 + w1\n    y1_max = y1 + h1\n\n    x2_min = x2\n    y2_min = y2\n    x2_max = x2 + w2\n    y2_max = y2 + h2\n\n    # Calculate coordinates of intersection rectangle\n    x_intersect_min = torch.max(x1_min, x2_min)\n    y_intersect_min = torch.max(y1_min, y2_min)\n    x_intersect_max = torch.min(x1_max, x2_max)\n    y_intersect_max = torch.min(y1_max, y2_max)\n\n    # Calculate area of intersection rectangle\n    intersect_width = torch.clamp(x_intersect_max - x_intersect_min, min=0)\n    intersect_height = torch.clamp(y_intersect_max - y_intersect_min, min=0)\n    intersect_area = intersect_width * intersect_height\n\n    # Calculate area of each bounding box\n    area1 = w1 * h1\n    area2 = w2 * h2\n\n    # Calculate union area\n    union_area = area1 + area2 - intersect_area\n\n    # Calculate IoU\n    iou = intersect_area / union_area\n\n    return iou\n```"}
{"id": "185", "AI response": "```python\nimport re\nimport json\n\ndef nginx_log_analysis(nginx_log_file, statistics_output_file, high_frequency_output_file):\n    \"\"\"\n    Analyzes Nginx access logs to count IP address accesses and identify high-frequency access.\n\n    Args:\n        nginx_log_file (str): Path to the Nginx access log file.\n        statistics_output_file (str): Path to the output file for statistics (JSON format).\n        high_frequency_output_file (str): Path to the output file for high-frequency IPs (JSON format).\n    \"\"\"\n\n    ip_counts = {}\n    high_frequency_ips = {}\n\n    try:\n        with open(nginx_log_file, 'r') as f:\n            for line in f:\n                # Extract IP address using regular expression\n                match = re.match(r'(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})', line)\n                if match:\n                    ip_address = match.group(1)\n                    ip_counts[ip_address] = ip_counts.get(ip_address, 0) + 1\n\n    except FileNotFoundError:\n        print(f\"Error: Log file not found: {nginx_log_file}\")\n        return\n\n    # Identify high-frequency IPs\n    for ip, count in ip_counts.items():\n        if count > 30:\n            high_frequency_ips[ip] = count\n\n    # Output statistics to JSON file\n    try:\n        with open(statistics_output_file, 'w') as f:\n            json.dump(ip_counts, f, indent=4)\n    except Exception as e:\n        print(f\"Error writing to statistics file: {e}\")\n\n    # Output high-frequency IPs to JSON file\n    try:\n        with open(high_frequency_output_file, 'w') as f:\n            json.dump(high_frequency_ips, f, indent=4)\n    except Exception as e:\n        print(f\"Error writing to high frequency file: {e}\")\n```"}
{"id": "186", "AI response": "```python\ndef lengthLongestPath(input):\n    stack = [0]\n    max_len = 0\n    for line in input.splitlines():\n        name = line.lstrip('\\t')\n        depth = len(line) - len(name)\n        while depth + 1 < len(stack):\n            stack.pop()\n        if '.' in name:\n            max_len = max(max_len, stack[-1] + len(name))\n        else:\n            stack.append(stack[-1] + len(name) + 1)\n    return max_len\n```"}
{"id": "187", "AI response": "```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef k_means_clustering(data, num_clusters):\n    \"\"\"\n    Performs K-means clustering on the given data.\n\n    Args:\n        data (np.ndarray): The input data array.\n        num_clusters (int): The number of clusters to form.\n\n    Returns:\n        np.ndarray: The cluster labels for each data sample.\n    \"\"\"\n    np.random.seed(42)\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)  # Explicitly set n_init\n    kmeans.fit(data)\n    return kmeans.labels_\n```"}
{"id": "188", "AI response": "```python\nimport cv2\nimport numpy as np\n\ndef gamma_correction(image, gamma):\n    \"\"\"\n    Applies gamma correction to an image.\n\n    Args:\n        image (numpy.ndarray): The input image.\n        gamma (float): The gamma value.\n\n    Returns:\n        numpy.ndarray: The gamma corrected image.\n    \"\"\"\n    # Normalize the image to the range [0, 1]\n    image_normalized = image / 255.0\n\n    # Apply gamma correction\n    corrected_image_normalized = np.power(image_normalized, gamma)\n\n    # Scale back to the range [0, 255] and convert to uint8\n    corrected_image = np.uint8(corrected_image_normalized * 255)\n\n    return corrected_image\n```"}
{"id": "189", "AI response": "```python\nimport os\nimport img2pdf\nfrom PIL import Image\nfrom PyPDF2 import PdfMerger\n\ndef merge_pdfs(root_dir):\n    \"\"\"\n    Merges all files in each subdirectory of root_dir into a single PDF file,\n    and places the merged PDF in the same subdirectory.\n    \"\"\"\n\n    for subdir, dirs, files in os.walk(root_dir):\n        if subdir == root_dir:\n            continue  # Skip the root directory itself\n\n        pdf_files = []\n        image_files = []\n        other_files = []\n\n        for file in files:\n            file_path = os.path.join(subdir, file)\n            file_ext = file.lower().split('.')[-1]\n\n            if file_ext == 'pdf':\n                pdf_files.append(file_path)\n            elif file_ext in ('jpg', 'jpeg', 'png'):\n                image_files.append(file_path)\n            else:\n                other_files.append(file_path)\n\n        # Convert images to PDF\n        converted_pdfs = []\n        for image_file in image_files:\n            try:\n                with Image.open(image_file) as img:\n                    image_pdf = img2pdf.convert(img.filename)\n                    temp_pdf_path = os.path.join(subdir, f\"{os.path.splitext(os.path.basename(image_file))[0]}.pdf\")\n                    with open(temp_pdf_path, \"wb\") as f:\n                        f.write(image_pdf)\n                    converted_pdfs.append(temp_pdf_path)\n            except Exception as e:\n                print(f\"Error converting image {image_file} to PDF: {e}\")\n\n        # Merge all PDF files\n        all_pdfs = pdf_files + converted_pdfs\n\n        if all_pdfs:\n            merger = PdfMerger()\n            for pdf_file in all_pdfs:\n                try:\n                    merger.append(pdf_file)\n                except Exception as e:\n                    print(f\"Error adding {pdf_file} to merger: {e}\")\n\n            output_path = os.path.join(subdir, \"merged.pdf\")\n            try:\n                merger.write(output_path)\n                merger.close()\n            except Exception as e:\n                print(f\"Error writing merged PDF to {output_path}: {e}\")\n\n            # Clean up temporary PDF files created from images\n            for temp_pdf in converted_pdfs:\n                try:\n                    os.remove(temp_pdf)\n                except Exception as e:\n                    print(f\"Error deleting temporary PDF {temp_pdf}: {e}\")\n```"}
{"id": "190", "AI response": "```python\nimport pandas as pd\nfrom io import StringIO\n\ndef calculate_date_difference(csv_contents):\n    \"\"\"\n    Calculates the difference in days between the earliest and latest date\n    in a CSV file represented as a string.\n\n    Args:\n        csv_contents (str): A string containing the contents of a CSV file,\n                           with a header row including a 'Date' column.\n\n    Returns:\n        int: The difference in days between the earliest and latest date.\n    \"\"\"\n    df = pd.read_csv(StringIO(csv_contents))\n    df['Date'] = pd.to_datetime(df['Date'])\n    min_date = df['Date'].min()\n    max_date = df['Date'].max()\n    return (max_date - min_date).days\n```"}
{"id": "191", "AI response": "```python\nimport os\nimport shutil\n\ndef copy_files(source_dir, dest_dir):\n    files = os.listdir(source_dir)\n    pairs = []\n    for file in files:\n        if file.endswith(\".bin\") or file.endswith(\".idx\"):\n            pairs.append(file)\n\n    pairs = sorted(pairs)\n    \n    num_pairs = len(pairs) // 2\n    num_subfolders = (num_pairs + 9) // 10\n    \n    for i in range(num_subfolders):\n        subfolder_name = os.path.join(dest_dir, f\"sub{i}\")\n        os.makedirs(subfolder_name, exist_ok=True)\n        \n        start_index = i * 10 * 2\n        end_index = min((i + 1) * 10 * 2, len(pairs))\n        \n        for j in range(start_index, end_index):\n            source_path = os.path.join(source_dir, pairs[j])\n            dest_path = os.path.join(subfolder_name, pairs[j])\n            shutil.copy2(source_path, dest_path)\n```"}
{"id": "192", "AI response": "```python\nimport re\n\ndef replace_code_block(text):\n    \"\"\"\n    Replaces code blocks like ```html ... ``` with <pre><code class=\"language-html\">...</code></pre>\n    \"\"\"\n    def replace(match):\n        language = match.group(1)\n        code = match.group(2)\n        return f'<pre><code class=\"language-{language}\">{code}</code></pre>'\n\n    pattern = r\"```(html|css|python|javascript|golang)\\s*([\\s\\S]*?)```\"\n    return re.sub(pattern, replace, text)\n```"}
{"id": "193", "AI response": "```python\nimport numpy as np\n\ndef calculate_et(albedo, solar_radiation, air_temperature, wind_speed, vapor_pressure, psychrometric_constant, latent_heat_of_vaporization, atmospheric_pressure):\n    \"\"\"\n    Calculates evapotranspiration (ET) using a simplified Penman-Monteith equation.\n\n    Args:\n        albedo (float): Albedo (surface reflectivity) [dimensionless].\n        solar_radiation (float): Solar radiation (Rs) [MJ m-2 day-1].\n        air_temperature (float): Air temperature (T) [°C].\n        wind_speed (float): Wind speed (u) [m/s].\n        vapor_pressure (float): Actual vapor pressure (ea) [kPa].\n        psychrometric_constant (float): Psychrometric constant (γ) [kPa °C-1].\n        latent_heat_of_vaporization (float): Latent heat of vaporization (λ) [MJ kg-1].\n        atmospheric_pressure (float): Atmospheric pressure (P) [kPa].\n\n    Returns:\n        float: Evapotranspiration (ET) [mm/day].\n    \"\"\"\n\n    # 1. Calculate net radiation (Rn)\n    Rn = (1 - albedo) * solar_radiation\n\n    # 2. Calculate saturation vapor pressure (es)\n    es = 0.6108 * np.exp((17.27 * air_temperature) / (air_temperature + 237.3))\n\n    # 3. Calculate slope of the saturation vapor pressure curve (Δ)\n    delta = (4098 * es) / ((air_temperature + 237.3)**2)\n\n    # 4. Penman-Monteith equation (simplified)\n    ET = (0.408 * delta * (Rn) + psychrometric_constant * (900 / (air_temperature + 273)) * wind_speed * (es - vapor_pressure)) / (delta + psychrometric_constant * (1 + 0.34 * wind_speed))\n\n\n    return ET\n```"}
{"id": "194", "AI response": "```python\nfrom lxml import etree\n\ndef has_strong_or_em(element):\n  \"\"\"\n  Determines if there are strong or em tags under the p tag text in python etree.\n\n  Args:\n    element: An etree element representing the p tag.\n\n  Returns:\n    True if there are strong or em tags under the p tag text, False otherwise.\n  \"\"\"\n  for child in element:\n    if child.tag == 'strong' or child.tag == 'em':\n      return True\n  return False\n```"}
{"id": "195", "AI response": "```python\nimport pandas as pd\n\ndef flatten_dict_to_rows(dt):\n  \"\"\"\n  Flattens a dictionary into a Pandas DataFrame where each key-value pair\n  occupies one row.\n\n  Args:\n    dt (dict): The dictionary to flatten.\n\n  Returns:\n    pd.DataFrame: A DataFrame with 'Key' and 'Value' columns.\n  \"\"\"\n  keys = list(dt.keys())\n  values = list(dt.values())\n  df = pd.DataFrame({'Key': keys, 'Value': values})\n  return df\n```"}
{"id": "196", "AI response": "```python\ndef is_mac_in_range(mac_address, mac_range):\n    \"\"\"\n    Determines whether a MAC address is within a given MAC address range.\n\n    Args:\n        mac_address: The MAC address to check (string).\n        mac_range: The MAC address range (string) in the format \"start_mac-end_mac\".\n\n    Returns:\n        True if the MAC address is within the range, False otherwise.\n    \"\"\"\n\n    try:\n        mac_address = mac_address.upper()\n        start_mac, end_mac = mac_range.upper().split('-')\n\n        mac_int = int(mac_address.replace(':', ''), 16)\n        start_int = int(start_mac.replace(':', ''), 16)\n        end_int = int(end_mac.replace(':', ''), 16)\n\n        return start_int <= mac_int <= end_int\n    except (ValueError, AttributeError):\n        return False\n```"}
{"id": "197", "AI response": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef fractional_order_model(t, R0, R1, R2, CPE1_T, CPE1_alpha, CPE2_T, CPE2_alpha, Capacity):\n    \"\"\"\n    Fractional order model with 2 CPEs and 3 resistors for a battery.\n\n    Args:\n        t (array-like): Time data.\n        R0 (float): Ohmic resistance.\n        R1 (float): Resistance of the first RC element.\n        R2 (float): Resistance of the second RC element.\n        CPE1_T (float): Constant Phase Element 1 parameter T.\n        CPE1_alpha (float): Constant Phase Element 1 parameter alpha.\n        CPE2_T (float): Constant Phase Element 2 parameter T.\n        CPE2_alpha (float): Constant Phase Element 2 parameter alpha.\n        Capacity (float): Battery capacity in Amp-hours.\n\n    Returns:\n        array-like: Voltage response of the battery.\n    \"\"\"\n    # This simplified model assumes constant current discharge\n\n    current = 1  #Assume 1A current for calculation.  User needs to scale result\n    voltage = R0 * current  # Immediate voltage drop\n\n    # Approximation of voltage drop over time for RC elements (simplified)\n    voltage += R1 * current * (1 - np.exp(-(t / (R1 * CPE1_T)**(1/CPE1_alpha))))\n    voltage += R2 * current * (1 - np.exp(-(t / (R2 * CPE2_T)**(1/CPE2_alpha))))\n\n    #Voltage drop related to capacity draw\n    voltage += (current * t) / (3600 * Capacity)\n    \n    return voltage\n\n\ndef estimate_capacity(time_data, voltage_data, initial_guess=None, bounds=None):\n    \"\"\"\n    Estimates the capacity of a battery using the fractional order model.\n\n    Args:\n        time_data (array-like): Time data in seconds.\n        voltage_data (array-like): Voltage data.\n        initial_guess (array-like, optional): Initial guess for the parameters\n            [R0, R1, R2, CPE1_T, CPE1_alpha, CPE2_T, CPE2_alpha, Capacity].\n            Defaults to None.\n        bounds (tuple, optional): Bounds for the parameters. Defaults to None.\n\n    Returns:\n        float: Estimated capacity in Amp-hours.\n    \"\"\"\n\n    if initial_guess is None:\n        initial_guess = [0.1, 0.1, 0.1, 0.1, 0.8, 0.1, 0.8, 1.0]  # Initial guess\n\n    if bounds is None:\n        bounds = (0, [1, 1, 1, 1, 1, 1, 1, 10])  # Reasonable bounds\n\n    try:\n        popt, pcov = curve_fit(fractional_order_model, time_data, voltage_data,\n                               p0=initial_guess, bounds=bounds, maxfev=10000)\n        \n        estimated_capacity = popt[-1]\n        return estimated_capacity\n\n    except Exception as e:\n        print(f\"Error during curve fitting: {e}\")\n        return None\n```"}
{"id": "198", "AI response": "```python\nimport numpy as np\n\ndef solve_quadratic(points):\n    \"\"\"\n    Solves for the coefficients a, b, and c of a quadratic function p(x) = ax^2 + bx + c\n    that passes through the given points.\n\n    Args:\n        points: A list of three tuples, where each tuple represents a point (x_i, y_i)\n                that the quadratic function passes through.\n\n    Returns:\n        A list containing the coefficients [a, b, c] of the quadratic function.\n    \"\"\"\n    x1, y1 = points[0]\n    x2, y2 = points[1]\n    x3, y3 = points[2]\n\n    # Construct the matrix A and the vector b for the system of linear equations\n    A = np.array([[x1**2, x1, 1],\n                  [x2**2, x2, 1],\n                  [x3**2, x3, 1]])\n    b = np.array([y1, y2, y3])\n\n    # Solve the system of linear equations using numpy.linalg.solve\n    coefficients = np.linalg.solve(A, b)\n\n    return coefficients.tolist()\n```"}
{"id": "199", "AI response": "```python\nimport ipaddress\n\ndef check_ip_overlap(range1, range2):\n    \"\"\"\n    Checks if two IP ranges overlap.\n\n    Args:\n        range1 (str): The first IP range in CIDR notation (e.g., '192.168.1.0/24').\n        range2 (str): The second IP range in CIDR notation (e.g., '192.168.2.0/24').\n\n    Returns:\n        bool: True if the IP ranges overlap, False otherwise.\n    \"\"\"\n    try:\n        net1 = ipaddress.ip_network(range1, strict=False)\n        net2 = ipaddress.ip_network(range2, strict=False)\n        return net1.overlaps(net2)\n    except ValueError:\n        return False\n```"}
{"id": "200", "AI response": "```python\nimport os\nimport re\n\ndef rename_files_in_folder(folder_path):\n    \"\"\"\n    Renames files in a folder by removing all letters from the filenames,\n    keeping only numbers and the file extension.\n\n    Args:\n        folder_path (str): The path to the folder containing the files to rename.\n    \"\"\"\n    for filename in os.listdir(folder_path):\n        if os.path.isfile(os.path.join(folder_path, filename)):\n            name, ext = os.path.splitext(filename)\n            new_name = re.sub(r'[^\\d]', '', name)\n            if new_name:\n                new_filename = new_name + ext\n                os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n```"}